\section{Optimering med Newton-Raphsons metod}

När ett ekvationssystem är ickelinjärt kan ej exakta metoder som Gausseliminering användas
för ekvationslösning. Detta stötte vi till exempel på vid finita elementlösningen
av Navier-Stokes ekvationer i avsnitt \ref{sec:femconvection}.
I dessa fall måste approximativa optimeringsmetoder utnyttjas. En sådan
metod är Newton-Raphsons metod. Denna bygger på trunkerad Taylorutveckling 
av en funktion för att linjarisera ett ickelinjärt ekvationssystem
$\mathbf{f}(\mathbf{x}) = 0$
vilket kan ses i ekvation \eqref{eq:newtonsmethod:taylor}. Här är
$\mathbf{J}_f(\mathbf{x})$ jacobianen för $\mathbf{f}(\mathbf{x})$. 

\begin{equation}
\label{eq:newtonsmethod:taylor}
\mathbf{f}(\mathbf{x} + \Delta\mathbf{x}) \approx \mathbf{f}(\mathbf{x}) +
\mathbf{J}_f(\mathbf{x})\Delta\mathbf{x}
\end{equation}

\noindent
Principen går ut på att algoritmen upprepat gissar nya lösningar där de
nya lösningarna följer den negativa jacobianen. Till en början är en god initial gissning
$\mathbf{x}_0$ ett kriterie för att Newton-Raphson skall konvergera. Därefter så beräknas
funktionsvärdet $\mathbf{f}(\mathbf{x}_0)$ samt jacobianen $\mathbf{J}_f(\mathbf{x}_0)$.
Dessa används för att beräkna nästa gissning genom att lösa
\eqref{eq:newtonsmethod:guess} och beräkna nästa $\mathbf{x}$ med
\eqref{eq:newtonsmethod:nextx}. \cite{heath2002}

\begin{equation}
\label{eq:newtonsmethod:guess}
\mathbf{J}_f(\mathbf{x}_n)\Delta\mathbf{x}_n = -\mathbf{f}(\mathbf{x_n})
\end{equation}

\begin{equation}
\label{eq:newtonsmethod:nextx}
\mathbf{x}_{n+1} = \mathbf{x}_n + \Delta\mathbf{x}_n
\end{equation}

\noindent
Itereringen bör avbrytas då ett maxantal itereringar har uppnåtts och funktionen
ej har konvergerat alternativt när felet är tillräckligt litet. En av styrkorna 
med denna algoritm är dess kvadratiska konvergens mot enkelrötter. \cite{ympa95}
En svaghet med Newton-Raphson är att det i många fall ej är möjligt att analytiskt beräkna
jacobianen. Istället måste andra algoritmer untnyttjas som till exempel finita differensmetoden
för beräkning av jacobianen. Omvägar som denna bidrar till att lösningsprocessen blir mycket
mer omständig och processorintensiv. 

\subsection{Konvergens samt konvergenskriterier}

För att enklare förstå några problem som kan uppstå med Newton-Raphsons metod kan det vara lämpligt
att repetera beviset av dess kvadratiska konvergens. Definiera en funktion $f(x)$ enligt \eqref{eq:newtonproof}.
Antag att den roten $f(x) = 0$ existerar för $x = \alpha$.

\begin{align}
f: & \mathbb{R} \to \mathbb{R} \nonumber \\
   & x \mapsto f(x) \label{eq:newtonproof}
\end{align}

\noindent
Härnäst genomförs en taylorutveckling av funktionen $f(x)$ i ekvation \eqref{eq:newtonprooftaylor}.
Den kvadratiska termen är här Lagranges restterm med parametern $\xi_n \in [\alpha, x_n]$.

\begin{equation}
\label{eq:newtonprooftaylor}
f(\alpha) = f(x_n) + f^\prime(x_n)(x_n-\alpha) + \frac{f^{\prime\prime}(\xi_n)}{2}(x_n-\alpha)^2
\end{equation}

\noindent
Nu kan förstaderivatan av $f(x_n)$ divideras över samt $f(\alpha)$ är känt att vara noll.
Efter detta identifieras $f(x_n)/f^\prime(x_n) = x_n-x_{n+1}$ och ersätts. Slutligen så ses
det i ekvation \eqref{eq:newtonqed} att $x_{n+1}-\alpha \propto (x_{n}-\alpha)^2$.

\begin{equation}
0 = \frac{f(x_n)}{f^\prime(x_n)} + \alpha - x_n + \frac{f^{\prime\prime}(\xi_n)}{2f^\prime(x_n)}(x_n-\alpha)^2
\Rightarrow
\end{equation}

\begin{equation}
\label{eq:newtonqed}
x_{n-1} - \alpha = - \frac{f^{\prime\prime}(\xi_n)}{2f^\prime(x_n)}(x_n-\alpha)^2 
\end{equation}

\noindent
För att ovanstående bevis ska gälla så måste andraderivatan vara uppåt begränsad, förstaderivatan får ej
vara noll och den högre ordningens derivator får ej vara av stor betydelse för funktionens uppträdande
nära roten $f(x) = 0$. Rent praktiskt innebär detta att en god gissning är essentiell för att få
konvergens i metoden. Även med en god gissning så kan problem uppstå om derivatan av funktionen
förändras snabbt i omgivningen av $x$. Detta kan resultera i både att nästa gissning ligger för långt
bort och för nära. Det förstnämnda problemet innebär att metoden hoppar över roten vilket till och med
kan innebära att metoden divergerar. Det andra problemet är mindre allvarligt då det endast innebär 
att metoden förlorar sin kvadratiska konvergens.

\subsection{Förbättrad Newton-Raphson}

En metod för att hantera att metoden hoppar över rötter är att i varje steg försöka minimera $|f(x_{n+1})|$.
Rent praktiskt innbär detta att vi väljer en konstant $0 \le k_n \le 1$ och genomför ett modifierat
Newtonsteg enligt ekvation \eqref{eq:newtonmodified}.

\begin{equation}
\label{eq:newtonmodified}
x_{n+1} = x_n - k_n\frac{f(x_n)}{f^\prime(x_n)}
\end{equation}

\noindent
För att identifiera det optimala valet av $k_n$ kan godtycklig linjesökningsalgoritm användas. Ett val av metod
är att behandla det analytiskt för att göra metoden mindre processorintensiv. En ny funktion definieras
enligt ekvation \eqref{eq:newtong} med $\Delta x_n = f(x_n)/f^\prime(x_n)$.

\begin{equation}
\label{eq:newtong}
g(k_n) = f(x_n- k_n\Delta x_n)
\end{equation}

\noindent
I ekvation \eqref{eq:newtongmin} deriveras funktionen med avseende på $k_n$ i punkten $k_n=0$
och kedjeregeln används för att skriva om uttrycket till något som är användbart.

\begin{align}
\frac{\partial g(k_n)}{\partial k_n}\,\bigg|_{k_n=0} & = 
\left(\frac{\partial g(k_n)}{\partial (k_n\Delta x_n)}
\frac{\partial k_n \Delta x_n}{\partial k_n}\right)\,\bigg|_{k_n=0} = \nonumber \\
x_n \frac{\partial f(x_n- k_n\Delta x_n)}{\partial (k_n\Delta x_n)}\,\bigg|_{k_n=0} & = 
-\Delta x_n f^\prime(x_n) = - f(x_n)
\label{eq:newtongmin}
\end{align}

\noindent
Ett förslag på algoritm är nu att i varje iterationssteg först beräkna det fulla newtonsteget motsvarande $k_n=1$.
Är då $f(x_{n+1}) < f(x_n)$ så kan steget godtagas. Stämmer inte detta så beräkas derivatan av $g(k_n)$ och
funktionen $g(k_n)$ ansätts vara ett polynom av andra ordningen enligt ekvation \eqref{eq:newtonfit}.

\begin{equation}
\label{eq:newtonfit}
g(k_n) = ak^2_n + bk_n + c
\end{equation}

\noindent
Nu kan de kända värdena $g(0)$, $g(1)$ samt $g^\prime(0)$ användas för att lösa ut koefficienterna i polynomet.
Dessa kan slutligen användas för att beräkna derivatan av $g(k_n)$ för att hitta dess minimum och således
hitta den optimala parametern $k_n$.

\noindent
Om denna metod skall användas för att lösa ett ekvationssystem istället för en realvärd funktion i en dimension
så behövs något mått sättas upp. Problemet som önskas att lösas är $\mathbf{F}(\mathbf{x}) = 0$.
Funktionen som skall minimeras kan då med fördel väljas till $f(\mathbf{x}) = \mathbf{F}(\mathbf{x})^2/2$.
På ett analogt sätt ovan så beräknas derivatan av funktionen $g(k)$ till ekvation \eqref{eq:newtonvecg}.\cite{fortran77}

\begin{equation}
\label{eq:newtonvecg}
g_n^\prime(0) = - \mathbf{F}(\mathbf{x_n})^2 \le 0
\end{equation}

\noindent
För att hitta roten $\mathbf{F}(\mathbf{x}) = 0$ så ansätts som ovan ett polynom där koefficienterna beräknas.
Som kan ses så existerar det ett $k_n$ så att $\mathbf{F}(\mathbf{x}_{n+1}) \le \mathbf{F}(\mathbf{x}_n)$ ty
$g^\prime(0) \le 0$ och enbart noll om roten redan är funnen.

