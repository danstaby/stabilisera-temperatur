\section{Optimering med Newton-Raphsons metod}

När ett ekvationssystem är ickelinjärt kan inga exakta metoder som gausseliminering användas
för ekvationslösning. Detta stötte vi till exempel på vid finita elementlösningen
av Navier-Stokes ekvationer i avsnitt~\ref{sec:femconvection}.
I dessa fall måste approximativa optimeringsmetoder utnyttjas. En sådan
metod är Newton-Raphsons metod. Denna bygger på trunkerad Taylorutveckling 
av en funktion för att linearisera ett ickelinjärt ekvationssystem
$\mathbf{f}(\mathbf{x}) = 0$
vilket kan ses i ekvation \eqref{eq:newtonsmethod:taylor}. Här är
$\mathbf{J}_f(\mathbf{x})$ jacobianen för $\mathbf{f}(\mathbf{x})$. 

\begin{equation}
\label{eq:newtonsmethod:taylor}
\mathbf{f}(\mathbf{x} + \Delta\mathbf{x}) \approx \mathbf{f}(\mathbf{x}) +
\mathbf{J}_f(\mathbf{x})\Delta\mathbf{x}
\end{equation}

\noindent
Principen går ut på att algoritmen upprepat gissar nya lösningar där de
nya lösningarna följer den negativa jacobianen. Till en början är en god initial gissning
$\mathbf{x}_0$ ett kriterium för att Newton-Raphsons metod skall konvergera. Därefter beräknas
funktionsvärdet $\mathbf{f}(\mathbf{x}_0)$ samt jacobianen $\mathbf{J}_f(\mathbf{x}_0)$.
Dessa används för att beräkna nästa gissning genom att lösa
\eqref{eq:newtonsmethod:guess} och beräkna nästa $\mathbf{x}$ med
\eqref{eq:newtonsmethod:nextx}. \cite{heath2002}

\begin{equation}
\label{eq:newtonsmethod:guess}
\mathbf{J}_f(\mathbf{x}_n)\Delta\mathbf{x}_n = -\mathbf{f}(\mathbf{x_n})
\end{equation}

\begin{equation}
\label{eq:newtonsmethod:nextx}
\mathbf{x}_{n+1} = \mathbf{x}_n + \Delta\mathbf{x}_n
\end{equation}

\noindent
Itereringen bör avbrytas när felet är tillräckligt litet, alternativ då ett största tillåtna antal itereringar har uppnåtts och funktionen inte har konvergerat. En av styrkorna 
med denna algoritm är dess kvadratiska konvergens mot enkelrötter. \cite{ympa95}
En svaghet hos Newton-Raphsons metod är att det i många fall inte är möjligt att analytiskt beräkna
jacobianen. Istället måste andra algoritmer utnyttjas för beräkning av jacobianen, som till exempel finita differensmetoden. Detta är omvägar som denna bidrar till att lösningsprocessen blir mer omständig och processorintensiv.

\subsection{Konvergens och konvergenskriterier}

För att enklare förstå några av de problem som kan uppstå med Newton-Raphsons metod kan det vara lämpligt
att repetera beviset för dess kvadratiska konvergens. Definiera en funktion $f(x)$ enligt \eqref{eq:newtonproof}.
Antag att den unika roten $f(x) = 0$ existerar för $x = \alpha$.

\begin{align}
f: & \mathbb{R} \to \mathbb{R} \nonumber \\
   & x \mapsto f(x) \label{eq:newtonproof}
\end{align}

\noindent
Sedan genomförs en taylorutveckling av funktionen $f(x)$ i 

\begin{equation}
\label{eq:newtonprooftaylor}
f(\alpha) = f(x_n) + f^\prime(x_n)(x_n-\alpha) + \frac{f^{\prime\prime}(\xi_n)}{2}(x_n-\alpha)^2.
\end{equation}

Den kvadratiska termen är Lagranges restterm med parametern $\xi_n \in [\alpha, x_n]$.

Efter detta divideras alla termer med $f^\prime(x_n)$. Det år också känt att $f(\alpha)=0$.
Därefter identifieras  $f(x_n)/f^\prime(x_n)$ som ett newtonsteg och ersätts enligt
$f(x_n)/f^\prime(x_n) = x_n-x_{n+1}$. Slutligen fås att $x_{n+1}-\alpha \propto (x_{n}-\alpha)^2$ ur

\begin{equation}
0 = \frac{f(x_n)}{f^\prime(x_n)} + \alpha - x_n + \frac{f^{\prime\prime}(\xi_n)}{2f^\prime(x_n)}(x_n-\alpha)^2
\Rightarrow
\end{equation}

\begin{equation}
\label{eq:newtonqed}
x_{n-1} - \alpha = - \frac{f^{\prime\prime}(\xi_n)}{2f^\prime(x_n)}(x_n-\alpha)^2 
\end{equation}

För att ovanstående bevis ska gälla måste andraderivatan vara uppåt begränsad, förstaderivatan måste vara nollskild och den högre ordningens derivator får inte vara av stor betydelse för funktionens beteende nära roten $f(x) = 0$. Rent praktiskt innebär detta att en god gissning är essentiell för att få
konvergens i metoden men även med en god gissning kan problem uppstå om derivatan av funktionen
förändras snabbt i $x$:s omgivning. Det kan resultera både i att nästa gissning ligger för långt
bort eller för nära från svaret. Det förstnämnda problemet leder till att metoden hoppar över roten vilket kan ge att metoden divergerar. Det andra problemet är mindre allvarligt men kan leda till att metoden förlorar sin kvadratiska konvergens.

\subsection{Förbättrad Newton-Raphson}

En åtgärd för att undvika att metoden hoppar över rötter är att i varje steg försöka minimera $|f(x_{n+1})|$.
Rent praktiskt innebär det att en konstant $0 \le k_n \le 1$ väljs och ett modifierat
newtonsteg genomförs enligt

\begin{equation}
\label{eq:newtonmodified}
x_{n+1} = x_n - k_n\frac{f(x_n)}{f^\prime(x_n)}.
\end{equation}

För att identifiera det optimala valet av $k_n$ kan en godtycklig linje\-söknings\-algoritm användas. Ett alternativ \emph{\color{red} till vad? Oklar syftning.}
är att behandla det analytiskt för att göra metoden mindre processor\-intensiv. En ny funktion definieras
enligt 

\begin{equation}
\label{eq:newtong}
g(k_n) = f(x_n- k_n\Delta x_n)
\end{equation}

med $\Delta x_n = f(x_n)/f^\prime(x_n)$.

Sedan bestämma funktionens derivata med avseende på $k_n$ i punkten $k_n=0$
och kedjeregeln används för att skriva om uttrycket till något som är användbart, alltså

\begin{align}
\frac{\partial g(k_n)}{\partial k_n}\,\bigg|_{k_n=0} & = 
\left(\frac{\partial g(k_n)}{\partial (k_n\Delta x_n)}
\frac{\partial k_n \Delta x_n}{\partial k_n}\right)\,\bigg|_{k_n=0} = \nonumber \\
\Delta x_n \frac{\partial f(x_n- k_n\Delta x_n)}{\partial (k_n\Delta x_n)}\,\bigg|_{k_n=0} & = 
-\Delta x_n f^\prime(x_n) = - f(x_n)
\label{eq:newtongmin}
\end{align}

Ett förslag på algoritm är att i varje iterations\-steg först beräkna det fulla newtonsteget, motsvarande $k_n=1$.
Är då $f(x_{n+1}) < f(x_n)$ kan steget godtas. Stämmer inte detta deriveras $g(k_n)$ och funktionen $g(k_n)$ ansätts att vara ett polynom av andra ordningen enligt

\begin{equation}
\label{eq:newtonfit}
g(k_n) = ak^2_n + bk_n + c.
\end{equation}

Nu kan de kända värdena $g(0)$, $g(1)$ och $g^\prime(0)$ användas för att lösa ut koefficienterna i polynomet.
Slutligen kan dessa användas för att beräkna derivatan av $g(k_n)$ och för att hitta dess minimum vilket ger den optimala parametern $k_n$.

Om denna metod ska användas för att lösa ett ekvationssystem istället för en realvärd funktion i en dimension behöver ett mått sättas upp. Problemet som ska lösas är $\mathbf{F}(\mathbf{x}) = 0$.
Funktionen som ska minimeras kan med fördel väljas till $f(\mathbf{x}) = \mathbf{F}(\mathbf{x})^2/2$.
På samma sätt som ovan beräknas derivatan av funktionen $g(k)$ till\cite{fortran77}

\begin{equation}
\label{eq:newtonvecg}
g_n^\prime(0) = - \mathbf{F}(\mathbf{x_n})^2 \le 0
\end{equation}

För att hitta roten $\mathbf{F}(\mathbf{x}) = 0$ ansätts som ovan ett polynom där koefficienterna beräknas.
Som kan ses så existerar det ett $k_n$ sådant att $\mathbf{F}(\mathbf{x}_{n+1}) \le \mathbf{F}(\mathbf{x}_n)$ eftersom
$g^\prime(0) \le 0$, och noll endast om roten redan är funnen.

